# configs/train_config.yaml

data:
  path: data/CelebA/  # Local Images Path
  parquet_path: None # /dbfs/mnt/ds-space/Hitesh/Datasets/CelebA-HQ/parquet_files/CelebA-HQ.parquet
  image_size: 256             
  normalize: true
  caption_path: data/captions.csv

checkpoint:
  path: Checkpoints/
  ckpt_name: UNet_ckpt_256.pth
  ema_ckpt_name: UNet_ema_ckpt_256.pth

training:
  batch_size: 4
  validation_split: 0
  epochs: 100
  warmup_epochs: 10
  lr: 1e-4
  grad_accum_steps: 2
  use_ema: true
  ema_beta: 0.995
  step_start_ema: 2000
  num_workers: 4
  prefetch_factor: 2

losses:
  lpips:
    net: alex

sampling:
  dir: output/samples/
  num_samples: 25
  steps: 50
  guidance_scale: 7.5

model:
  type: unet
  sample_size: 32                  # 256 / 8 = 32 latent resolution
  in_channels: 4                   # VAE latent channels
  out_channels: 4                  # same as in_channels for noise prediction
  block_out_channels: [256, 512, 1024, 1024]  # typical SD1.5 backbone: [320, 640, 1280, 1280]. Low setting: [256, 512, 1024, 1024]
  down_block_types:
    - CrossAttnDownBlock2D         # 32x32 → 16x16
    - CrossAttnDownBlock2D         # 16x16 → 8x8
    - CrossAttnDownBlock2D         # 8x8 → 4x4
    - DownBlock2D                  # 4x4 → 2x2
  up_block_types:
    - UpBlock2D                    # 2x2 → 4x4
    - CrossAttnUpBlock2D           # 4x4 → 8x8
    - CrossAttnUpBlock2D           # 8x8 → 16x16
    - CrossAttnUpBlock2D           # 16x16 → 32x32
  layers_per_block: 2              # 2 resblocks per stage
  cross_attention_dim: 768         # CLIP ViT-L/14 embedding size
  attention_head_dim: 8            # matches SD1.5 defaults
  act_fn: silu                     # smooth activation function
  norm_eps: 1e-5                   # layernorm epsilon
  norm_num_groups: 32             # for GroupNorm
  center_input_sample: false
  downsample_padding: 1
  flip_sin_to_cos: true
  freq_shift: 0
  mid_block_scale_factor: 1

scheduler:
  type: squaredcos_cap_v2
  timesteps: 1000
  beta_start: 0.0001
  beta_end: 0.02

logs: "logs"
