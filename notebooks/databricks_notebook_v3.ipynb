{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57080a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\\Projects\\Diffusion Model\\UNet-Diffusion\n"
     ]
    }
   ],
   "source": [
    "# change working dir\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d864e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade torch diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a837d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ba1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc8e5071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download dataset\n",
    "# %sh\n",
    "# gdown --folder https://drive.google.com/drive/u/1/folders/1Ak8sYUszWhoLWvVzv-s9HNxMbdI0sAiV -O /dbfs/mnt/ds-space/Hitesh/Datasets/ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1923983a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "\n",
    "from utils.ema import create_ema_model\n",
    "from utils.checkpoint import save_training_state, load_training_state\n",
    "from utils.celeba_parquet_dataset import DatasetLoader\n",
    "from utils.metrics.gpu import init_nvml, gpu_info\n",
    "from omegaconf import OmegaConf\n",
    "from utils.generate_samples import generate_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea709f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Mixed precision training enabled\n",
      "Configuration loaded: data:\n",
      "  path: data/images/\n",
      "  parquet_path: data/celebA_high.parquet\n",
      "  image_size: 256\n",
      "  normalize: true\n",
      "  caption_path: data/captions.jsonl\n",
      "checkpoint:\n",
      "  path: Checkpoints/\n",
      "  ckpt_name: UNet_ckpt_256.pth\n",
      "  ema_ckpt_name: UNet_ema_ckpt_256.pth\n",
      "training:\n",
      "  batch_size: 4\n",
      "  validation_split: 0\n",
      "  epochs: 10\n",
      "  lr: 0.0001\n",
      "  grad_accum_steps: 2\n",
      "  use_ema: true\n",
      "  ema_beta: 0.995\n",
      "  step_start_ema: 2000\n",
      "  num_workers: 4\n",
      "  prefetch_factor: 2\n",
      "  use_lpips: false\n",
      "  warmup_epochs: 5\n",
      "losses:\n",
      "  lpips:\n",
      "    net: vgg\n",
      "sampling:\n",
      "  dir: output/samples/\n",
      "  num_samples: 25\n",
      "  steps: 50\n",
      "  guidance_scale: 7.5\n",
      "model:\n",
      "  type: unet\n",
      "  sample_size: 32\n",
      "  in_channels: 4\n",
      "  out_channels: 4\n",
      "  block_out_channels:\n",
      "  - 256\n",
      "  - 512\n",
      "  - 1024\n",
      "  - 1024\n",
      "  down_block_types:\n",
      "  - CrossAttnDownBlock2D\n",
      "  - CrossAttnDownBlock2D\n",
      "  - CrossAttnDownBlock2D\n",
      "  - DownBlock2D\n",
      "  up_block_types:\n",
      "  - UpBlock2D\n",
      "  - CrossAttnUpBlock2D\n",
      "  - CrossAttnUpBlock2D\n",
      "  - CrossAttnUpBlock2D\n",
      "  layers_per_block: 2\n",
      "  cross_attention_dim: 768\n",
      "scheduler:\n",
      "  type: squaredcos_cap_v2\n",
      "  timesteps: 1000\n",
      "  beta_start: 0.0001\n",
      "  beta_end: 0.02\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "torch.manual_seed(1)\n",
    "handle = init_nvml()\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Enable mixed precision training\n",
    "scaler = torch.amp.GradScaler(\"cuda\") if device == \"cuda\" else None\n",
    "print(\"Mixed precision training enabled\" if device == \"cuda\" else \"Mixed precision training disabled\")\n",
    "\n",
    "# Load configuration\n",
    "config = OmegaConf.load(\"configs/config_databricks_256.yaml\")\n",
    "# config = OmegaConf.load(\"configs/train_config_256.yaml\")\n",
    "print(f\"Configuration loaded: {OmegaConf.to_yaml(config)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458f258e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Models, optimizers, losses initialized successfully ===\n"
     ]
    }
   ],
   "source": [
    "# Load VAE\n",
    "vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").to(device).eval()\n",
    "for p in vae.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Load UNet\n",
    "model = UNet2DConditionModel(\n",
    "    sample_size=config.model.sample_size,\n",
    "    in_channels=config.model.in_channels,\n",
    "    out_channels=config.model.out_channels,\n",
    "    down_block_types=config.model.down_block_types,\n",
    "    up_block_types=config.model.up_block_types,\n",
    "    block_out_channels=config.model.block_out_channels,\n",
    "    layers_per_block=config.model.layers_per_block,\n",
    "    cross_attention_dim=config.model.cross_attention_dim,\n",
    ").to(device)\n",
    "\n",
    "# Noise scheduler\n",
    "scheduler = DDPMScheduler(\n",
    "    num_train_timesteps=config.scheduler.timesteps,\n",
    "    beta_start=config.scheduler.beta_start,\n",
    "    beta_end=config.scheduler.beta_end,\n",
    "    beta_schedule=config.scheduler.type,\n",
    ")\n",
    "\n",
    "# CLIP tokenizer & encoder\n",
    "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "clip_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device).eval()\n",
    "\n",
    "# EMA\n",
    "unet_ema_model, ema = create_ema_model(model, beta=config.training.ema_beta, step_start_ema=config.training.step_start_ema)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.training.lr)\n",
    "MSE_LOSS = torch.nn.MSELoss()\n",
    "use_lpips = config.training.use_lpips\n",
    "if use_lpips:\n",
    "    import lpips\n",
    "    LPIPS_LOSS   = lpips.LPIPS(net=config.losses.lpips).to(device).eval() # net=vgg or alex\n",
    "\n",
    "print(\"=== Models, optimizers, losses initialized successfully ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c55c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset\n",
      "Total Images: 12288, batch size: 4\n",
      "‚ö†Ô∏è Checkpoint not found at Checkpoints/UNet_ckpt_256.pth. Starting from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 2/3072 [00:11<4:52:27,  5.72s/it, GPU=üö®GPU usage:12170 > 11900 Mib), avg_loss=0.543, avg_lpips=0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# ---- Noise Prediction ----\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(device_type=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, enabled=(device == \u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)):\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     noise_pred = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m)\u001b[49m.sample\n\u001b[32m     57\u001b[39m     mse_loss = MSE_LOSS(noise_pred, noise) / config.training.grad_accum_steps\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_lpips:\n\u001b[32m     60\u001b[39m         \u001b[38;5;66;03m# lpips weight\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_condition.py:1214\u001b[39m, in \u001b[36mUNet2DConditionModel.forward\u001b[39m\u001b[34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[39m\n\u001b[32m   1211\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_adapter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(down_intrablock_additional_residuals) > \u001b[32m0\u001b[39m:\n\u001b[32m   1212\u001b[39m         additional_residuals[\u001b[33m\"\u001b[39m\u001b[33madditional_residuals\u001b[39m\u001b[33m\"\u001b[39m] = down_intrablock_additional_residuals.pop(\u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1214\u001b[39m     sample, res_samples = \u001b[43mdownsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[43m=\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_residuals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1224\u001b[39m     sample, res_samples = downsample_block(hidden_states=sample, temb=emb)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\diffusers\\models\\unets\\unet_2d_blocks.py:1270\u001b[39m, in \u001b[36mCrossAttnDownBlock2D.forward\u001b[39m\u001b[34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask, additional_residuals)\u001b[39m\n\u001b[32m   1268\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1269\u001b[39m     hidden_states = resnet(hidden_states, temb)\n\u001b[32m-> \u001b[39m\u001b[32m1270\u001b[39m     hidden_states = \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1271\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1272\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1273\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1274\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1275\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m   1279\u001b[39m \u001b[38;5;66;03m# apply additional residuals to the output of the last pair of resnet and attention blocks\u001b[39;00m\n\u001b[32m   1280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[38;5;28mlen\u001b[39m(blocks) - \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m additional_residuals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\diffusers\\models\\transformers\\transformer_2d.py:404\u001b[39m, in \u001b[36mTransformer2DModel.forward\u001b[39m\u001b[34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[39m\n\u001b[32m    402\u001b[39m     batch_size, _, height, width = hidden_states.shape\n\u001b[32m    403\u001b[39m     residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m404\u001b[39m     hidden_states, inner_dim = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_operate_on_continuous_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_input_vectorized:\n\u001b[32m    406\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.latent_image_embedding(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\diffusers\\models\\transformers\\transformer_2d.py:466\u001b[39m, in \u001b[36mTransformer2DModel._operate_on_continuous_inputs\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_operate_on_continuous_inputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[32m    465\u001b[39m     batch, _, height, width = hidden_states.shape\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_linear_projection:\n\u001b[32m    469\u001b[39m         hidden_states = \u001b[38;5;28mself\u001b[39m.proj_in(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:313\u001b[39m, in \u001b[36mGroupNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Incognito-R\\miniconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\nn\\functional.py:2965\u001b[39m, in \u001b[36mgroup_norm\u001b[39m\u001b[34m(input, num_groups, weight, bias, eps)\u001b[39m\n\u001b[32m   2958\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2959\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m.dim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2960\u001b[39m     )\n\u001b[32m   2961\u001b[39m _verify_batch_size(\n\u001b[32m   2962\u001b[39m     [\u001b[38;5;28minput\u001b[39m.size(\u001b[32m0\u001b[39m) * \u001b[38;5;28minput\u001b[39m.size(\u001b[32m1\u001b[39m) // num_groups, num_groups]\n\u001b[32m   2963\u001b[39m     + \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m.size()[\u001b[32m2\u001b[39m:])\n\u001b[32m   2964\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2965\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2966\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\n\u001b[32m   2967\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(\"logs/training.log\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Data\n",
    "print(f\"Loading Dataset\")\n",
    "dataloader, _ = DatasetLoader(data_config=config.data, train_config=config.training, device=device)\n",
    "print(f\"Total Images: {len(dataloader.dataset)}, batch size: {dataloader.batch_size}\")\n",
    "\n",
    "# batch = next(iter(dataloader))\n",
    "# print(f\"Batch images shape: {batch['image'].shape}, Batch captions: {len(batch['text'])}, Batch images path: {len(batch['image_id'])}\")\n",
    "\n",
    "# Checkpoint\n",
    "os.makedirs(config.checkpoint.path, exist_ok=True)\n",
    "ckpt_path = os.path.join(config.checkpoint.path, config.checkpoint.ckpt_name)\n",
    "start_epoch, best_loss = load_training_state(ckpt_path, model, optimizer, device)\n",
    "\n",
    "# === Training loop ===\n",
    "warmup_ep = config.training.warmup_epochs\n",
    "for epoch in range(start_epoch, config.training.epochs + 1):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    cumm_loss = 0.0\n",
    "    cumm_mse = 0.0\n",
    "    cumm_lpips = 0.0\n",
    "\n",
    "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch}/{config.training.epochs}\")\n",
    "\n",
    "    for batch_idx, batch in pbar:\n",
    "        if batch_idx % config.training.grad_accum_steps == 0:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        images = batch['image'].to(device).float()\n",
    "        captions = batch['text']\n",
    "        text_inputs = clip_tokenizer( captions, padding=\"max_length\", truncation=True, max_length=35, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # ---- VAE Encoding ----\n",
    "            latents = vae.encode(images).latent_dist.sample() * 0.18215 \n",
    "            text_embeddings = clip_encoder(**text_inputs).last_hidden_state\n",
    "\n",
    "        t = torch.randint(0, scheduler.config.num_train_timesteps, (latents.size(0),), device=device)\n",
    "        noise = torch.randn_like(latents)\n",
    "        x_t = scheduler.add_noise(latents, noise, t)\n",
    "\n",
    "        # ---- Noise Prediction ----\n",
    "        with torch.amp.autocast(device_type='cuda', enabled=(device == 'cuda')):\n",
    "            noise_pred = model(x_t, timestep=t, encoder_hidden_states=text_embeddings).sample\n",
    "            mse_loss = MSE_LOSS(noise_pred, noise) / config.training.grad_accum_steps\n",
    "\n",
    "            if use_lpips:\n",
    "                # lpips weight\n",
    "                if epoch <= warmup_ep:\n",
    "                    lpips_weight = 0.0\n",
    "                else:\n",
    "                    # ramp-to-0.05 over [warmup_ep+1 .. 30], then hold\n",
    "                    frac = min((epoch - warmup_ep) / float(30 - warmup_ep), 1.0)\n",
    "                    lpips_weight = 0.05 * frac\n",
    "                # lpips loss\n",
    "                if lpips_weight > 0:\n",
    "                    alpha_t = scheduler.alphas_cumprod[t].view(-1, 1, 1, 1).clamp(min=1e-7)\n",
    "                    pred_x0 = (x_t - (1 - alpha_t).sqrt() * noise_pred) / alpha_t.sqrt()\n",
    "                    normed_x0 = torch.nan_to_num(pred_x0 / 0.18215) # root cause\n",
    "                    # normed_x0 = normed_x0.clamp(-6, 6)  # This solves the issue if nan in lpips but need optimum range for normed_x0 if any issue\n",
    "\n",
    "                    with torch.no_grad():\n",
    "                        with torch.amp.autocast(device_type='cuda', enabled=False):\n",
    "                            pred_rgb = vae.decode(normed_x0).sample.clamp(-1, 1)\n",
    "\n",
    "                    if torch.isnan(pred_rgb).any() or torch.isinf(pred_rgb).any():\n",
    "                        print(f\"[FATAL] pred_rgb exploded at epoch {epoch}, step {batch_idx+1}\")\n",
    "                        print(f\"normed_x0 stats: min={normed_x0.min():.2f}, max={normed_x0.max():.2f}, std={normed_x0.std():.2f}\")\n",
    "                        raise ValueError(\"pred_rgb contains NaNs or Infs\")\n",
    "\n",
    "                    lpips_loss =  LPIPS_LOSS(pred_rgb, images).mean()\n",
    "                else:\n",
    "                    lpips_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "                total_loss = mse_loss + lpips_weight * lpips_loss\n",
    "            \n",
    "            else:\n",
    "                total_loss = mse_loss\n",
    "\n",
    "        # ---- Backward Pass ----\n",
    "        scaler.scale(total_loss).backward() # Overall loss\n",
    "\n",
    "        # ---- Gradient Accumulation ----\n",
    "        if (batch_idx + 1) % config.training.grad_accum_steps == 0:\n",
    "            if device == 'cuda':\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                ema.step_ema(unet_ema_model, model)\n",
    "                \n",
    "        # ---- Progress Tracking ----\n",
    "        if use_lpips:\n",
    "            cumm_lpips += lpips_loss.item() if isinstance(lpips_loss, float) else lpips_loss.item()\n",
    "            avg_lpips = cumm_lpips / (batch_idx + 1)\n",
    "        \n",
    "        cumm_mse += mse_loss.item()\n",
    "        avg_mse = cumm_mse / (batch_idx + 1)\n",
    "        cumm_loss += total_loss.item() # main\n",
    "        avg_loss = cumm_loss / (batch_idx + 1) # Total average loss\n",
    "        best_loss = min(best_loss, avg_loss)\n",
    "        avg_lpips = avg_lpips if use_lpips else 0.0\n",
    "\n",
    "        pbar.set_postfix(avg_loss = avg_loss, avg_lpips = avg_lpips, GPU=gpu_info(handle))\n",
    "    \n",
    "        if (batch_idx+1) % 1 == 0:\n",
    "            # normed_x0 min/max: {normed_x0.min().item():.3f}/{normed_x0.max().item():.3f} # Add logging for debugging\n",
    "            logging.info(f\"Epoch: {epoch} Batch: {batch_idx+1} | AVG MSE: {avg_mse:.4f} | AVG lpips: {avg_lpips:.4f} | AVG Total: {avg_loss:.4f}\")\n",
    "\n",
    "        # Epoch summary logging\n",
    "    avg_loss = cumm_loss / (batch_idx + 1)\n",
    "    print(f\"Epoch {epoch} done. Avg loss: {avg_loss:.4f}\")\n",
    "\n",
    "    print(\"SAVING MODEL STATES...\")\n",
    "    # Save checkpoint & EMA weights\n",
    "    save_training_state(\n",
    "        checkpoint_path=ckpt_path, epoch=epoch,\n",
    "        model=model, optimizer=optimizer,\n",
    "        avg_loss=avg_loss, best_loss=best_loss\n",
    "    )\n",
    "    print(\"UNET MODEL SAVED!\")\n",
    "\n",
    "    ema_path = os.path.join(config.checkpoint.path, config.checkpoint.ema_ckpt_name)\n",
    "    torch.save(unet_ema_model.state_dict(), ema_path)\n",
    "    print(\"EMA_UNET MODEL SAVED!\")\n",
    "\n",
    "    # Generate visual sample for this epoch\n",
    "    generate_sample(\n",
    "        epoch=epoch, vae=vae, ema_model=unet_ema_model,\n",
    "        scheduler=scheduler, tokenizer=clip_tokenizer, text_encoder=clip_encoder,\n",
    "        config=config, device=device\n",
    "    )\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f10b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "717f022e",
   "metadata": {},
   "source": [
    "# Sample-Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fc4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from tqdm import tqdm\n",
    "\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from omegaconf import OmegaConf\n",
    "from utils.ema import create_ema_model\n",
    "from utils.metrics.gpu import init_nvml, gpu_info\n",
    "\n",
    "@torch.no_grad()\n",
    "def main():\n",
    "    # Load configuration\n",
    "    config = OmegaConf.load(\"configs/train_config_256.yaml\")\n",
    "    model_cfg = config.model\n",
    "    sample_cfg = config.sampling\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    handle = init_nvml()\n",
    "\n",
    "    # Prepare output\n",
    "    os.makedirs(sample_cfg.dir, exist_ok=True)\n",
    "    output_path = os.path.join(sample_cfg.dir, 'test_sample_grid.png')\n",
    "\n",
    "    # Load VAE\n",
    "    vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\").to(device).eval()\n",
    "\n",
    "    # Load conditional U-Net\n",
    "    model = UNet2DConditionModel(\n",
    "        sample_size=model_cfg.sample_size,\n",
    "        in_channels=model_cfg.in_channels,\n",
    "        out_channels=model_cfg.out_channels,\n",
    "        down_block_types=tuple(model_cfg.down_block_types),\n",
    "        up_block_types=tuple(model_cfg.up_block_types),\n",
    "        block_out_channels=tuple(model_cfg.block_out_channels),\n",
    "        layers_per_block=model_cfg.layers_per_block,\n",
    "        cross_attention_dim=model_cfg.cross_attention_dim,\n",
    "    ).to(device)\n",
    "\n",
    "    # EMA wrapper\n",
    "    ema_model, _ = create_ema_model(\n",
    "        model,\n",
    "        beta=config.training.ema_beta,\n",
    "        step_start_ema=config.training.step_start_ema\n",
    "    )\n",
    "    ema_ckpt = os.path.join(config.checkpoint.path, config.checkpoint.ema_ckpt_name)\n",
    "    ema_model.load_state_dict(torch.load(ema_ckpt, map_location=device))\n",
    "    ema_model.eval()\n",
    "\n",
    "    # Scheduler aligned with training\n",
    "    scheduler = DDPMScheduler(\n",
    "        num_train_timesteps=config.scheduler.timesteps,\n",
    "        beta_start=config.scheduler.beta_start,\n",
    "        beta_end=config.scheduler.beta_end,\n",
    "        beta_schedule=config.scheduler.type\n",
    "    )\n",
    "    scheduler.set_timesteps(sample_cfg.steps)\n",
    "\n",
    "    # Load CLIP\n",
    "    tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')\n",
    "    text_encoder = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14').to(device).eval()\n",
    "\n",
    "    # Prepare guidance embeddings\n",
    "    prompt = \"a beautiful woman in a red dress\"\n",
    "    text_inputs = tokenizer(\n",
    "        [prompt] * sample_cfg.num_samples,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=77,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    text_emb = text_encoder(**text_inputs).last_hidden_state\n",
    "    # Unconditional (empty) embeddings\n",
    "    uncond_inputs = tokenizer(\n",
    "        [\"\"] * sample_cfg.num_samples,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=77,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    uncond_emb = text_encoder(**uncond_inputs).last_hidden_state\n",
    "\n",
    "    # Sampling\n",
    "    num_samples = sample_cfg.num_samples\n",
    "    shape = (num_samples, model_cfg.in_channels, model_cfg.sample_size, model_cfg.sample_size)\n",
    "    latents = torch.randn(shape, device=device)\n",
    "    guidance_scale = sample_cfg.guidance_scale if hasattr(sample_cfg, 'guidance_scale') else 7.5\n",
    "\n",
    "    for t in tqdm(scheduler.timesteps, desc=\"Sampling\"):  # timesteps descends\n",
    "        t_batch = torch.full((num_samples,), t, device=device, dtype=torch.long)\n",
    "\n",
    "        # Predict noise for both conditional and unconditional\n",
    "        eps_uncond = ema_model(latents, timestep=t_batch, encoder_hidden_states=uncond_emb).sample\n",
    "        eps_cond   = ema_model(latents, timestep=t_batch, encoder_hidden_states=text_emb).sample\n",
    "        # Classifier-free guidance\n",
    "        eps = eps_uncond + guidance_scale * (eps_cond - eps_uncond)\n",
    "\n",
    "        # Step\n",
    "        latents = scheduler.step(eps, t, latents).prev_sample\n",
    "\n",
    "    # Decode latents to images\n",
    "    images = vae.decode(latents / 0.18215).sample\n",
    "    images = (images.clamp(-1, 1) + 1) / 2\n",
    "\n",
    "    # Save grid\n",
    "    grid = make_grid(images, nrow=int(num_samples**0.5))\n",
    "    save_image(grid, output_path)\n",
    "    print(f\"‚úÖ Samples saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.cuda.empty_cache()\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
